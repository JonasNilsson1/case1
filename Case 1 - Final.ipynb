{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing as preproc # load preprocessing function\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as colors\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
    "import warnings # to silence convergence warnings\n",
    "\n",
    "# seaborn can be used to \"prettify\" default matplotlib plots by importing and setting as default\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default\n",
    "\n",
    "#being used to check dir and change of dir\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to load all missing values to the same format\n",
    "missing_values = [\"n.a.\",\"NA\",\"n/a\", \"na\", \" NaN\",\"NaN\"]\n",
    "df = pd.read_csv('case1Data.txt', delimiter = \",\" ,na_values=missing_values, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove space in heading\n",
    "df.columns = df.columns.str.replace('_ ','_').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting response and predictors from each other.\n",
    "y, X = df['y'], df.drop(['y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to load all missing values to the same format\n",
    "missing_values = [\"n.a.\",\"NA\",\"n/a\", \"na\", \" NaN\",\"NaN\"]\n",
    "df_pred = pd.read_csv('case1Data_Xnew.txt', delimiter = \",\",na_values=missing_values, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove space in heading\n",
    "df_pred.columns = df_pred.columns.str.replace('_ ','_').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "### Missing values - Training Data\n",
    "There are multiple ways to handle missing values. But the simplest one is simply to one-hot-encode the categorical variables, where NaN also will be a variable. \n",
    "\n",
    "https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start by locating where the missing values are\n",
    "columns_with_missing_values = X.columns[X.isnull().any()]\n",
    "X[columns_with_missing_values].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To hold variable names\n",
    "labels = [] \n",
    "\n",
    "# To hold the count of missing values for each variable \n",
    "valuecount = [] \n",
    "\n",
    "# To hold the percentage of missing values for each variable\n",
    "percentcount = [] \n",
    "\n",
    "for col in columns_with_missing_values:\n",
    "    labels.append(col)\n",
    "    valuecount.append(X[col].isnull().sum())\n",
    "    # X.shape[0] will give the total row count\n",
    "    percentcount.append(X[col].isnull().sum()/X.shape[0])\n",
    "\n",
    "ind = np.arange(len(labels))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,18))\n",
    "\n",
    "rects = ax1.barh(ind, np.array(valuecount), color='blue')\n",
    "ax1.set_yticks(ind)\n",
    "ax1.set_yticklabels(labels, rotation='horizontal')\n",
    "ax1.set_xlabel(\"Count of missing values\")\n",
    "ax1.set_title(\"Variables with missing values\")\n",
    "\n",
    "rects = ax2.barh(ind, np.array(percentcount), color='pink')\n",
    "ax2.set_yticks(ind)\n",
    "ax2.set_yticklabels(labels, rotation='horizontal')\n",
    "ax2.set_xlabel(\"Percentage of missing values\")\n",
    "ax2.set_title(\"Variables with missing values\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above illustrates the counts and the percentage of each of the predictors, where there are missing values. For both C1 and C2 there are 15 missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import seaborn. We will use seaborn to generate our charts\n",
    "import seaborn as sns\n",
    "\n",
    "# We will import matplotlib to resize our plot figure\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# cubehelix palette is a part of seaborn that produces a colormap\n",
    "cmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\n",
    "sns.heatmap(df.isnull(), cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis above, we can see that we're only missing values in the categorical variables. Hence why, use subsection 9.2.4 in the book. This subsection describes how to treat missing values for categorical variables for three-based methods.\n",
    "\n",
    "\n",
    "It states the following : \"The first is applicable to categorical predictors: we simply make a new category for “missing.” From this we might discover that observations with missing values for some measurement behave differently than those with nonmissing values.\"\n",
    "\n",
    "Reasons why we don't one-hot: https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.fillna(value=\"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = preproc.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning numerical values and storing in another column\n",
    "X['C_1'] = labelencoder.fit_transform(X['C_1'])\n",
    "X['C_2'] = labelencoder.fit_transform(X['C_2'])\n",
    "X['C_3'] = labelencoder.fit_transform(X['C_3'])\n",
    "X['C_4'] = labelencoder.fit_transform(X['C_4'])\n",
    "X['C_5'] = labelencoder.fit_transform(X['C_5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same is done for the other dataset, that is used for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start by locating where the missing values are\n",
    "columns_with_missing_values = df_pred.columns[df_pred.isnull().any()]\n",
    "df_pred[columns_with_missing_values].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_pred.fillna(value=\"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning numerical values and storing in another column\n",
    "df_pred['C_1'] = labelencoder.fit_transform(df_pred['C_1'])\n",
    "df_pred['C_2'] = labelencoder.fit_transform(df_pred['C_2'])\n",
    "df_pred['C_3'] = labelencoder.fit_transform(df_pred['C_3'])\n",
    "df_pred['C_4'] = labelencoder.fit_transform(df_pred['C_4'])\n",
    "df_pred['C_5'] = labelencoder.fit_transform(df_pred['C_5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis\n",
    "\n",
    "This is done so we have an understanding of the dataset. This can be done in multiple steps as\n",
    "\n",
    "- Data Cleaning\n",
    "- Multivariate analysis\n",
    "\n",
    "Some of the data cleaning is done above, but lets investigate if further data cleaning is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multivariate outliers\n",
    "ax = sns.boxplot(x=df[\"x_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "c = df.corr()\n",
    "sns.heatmap(c,cmap=\"BrBG\",annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().unstack().sort_values(ascending = False).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difficult to say anything from this correlation plot. It can be spotted there there is a negative correlation with the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, density=True)  # density=False would make counts\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost a normal distribution, migth cause some troubles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "There have been chosen to do a two-fold cross-validation\n",
    "\n",
    "https://towardsdatascience.com/nested-cross-validation-hyperparameter-optimization-and-model-selection-5885d84acda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the cross-validation\n",
    "K = 5\n",
    "CV = KFold(K, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inner loop is defined below, this is where we're doing our model selection. \n",
    "The EPE is calculated within the GridSearchCV, where RMSE is used. When calling the function .best_score_, the out is the mean of RMSE from all the folds, which is eqaclty when we want to determine which model to go with when determine the final model and the model assesment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models(X, y, K):\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    InnerCV = KFold(K, shuffle=True)\n",
    "    \n",
    "    ##############################\n",
    "    ########### MODLES ###########\n",
    "    ##############################\n",
    "    \n",
    "    # Bagging\n",
    "    model_bag = BaggingRegressor()\n",
    "    \n",
    "    params_bag = {'n_estimators': [20,50,100],\n",
    "                  'max_samples': [0.5,1.0, n_samples//2,],\n",
    "                  'max_features': [0.5,1.0, n_features//2,],\n",
    "                  'bootstrap': [True, False],\n",
    "                  'bootstrap_features': [True, False]}\n",
    "    \n",
    "    bag_grid = GridSearchCV(model_bag, params_bag, cv=InnerCV, scoring='neg_root_mean_squared_error').fit(X,y)\n",
    "    best_bag = bag_grid.best_estimator_\n",
    "    \n",
    "    EPE_bag = bag_grid.best_score_ #this value is given as the mean\n",
    "    print('Done with Bagging')\n",
    "    \n",
    "    # Ada Boost\n",
    "    model_ada = AdaBoostRegressor()\n",
    "    \n",
    "    params_ada = parameters = {'n_estimators':[10, 50, 100, 500], \n",
    "                               'learning_rate':[0.0001, 0.001, 0.01, 0.1, 1.0]}\n",
    "    ada_grid = GridSearchCV(model_ada, params_ada, cv=InnerCV, scoring='neg_root_mean_squared_error').fit(X,y)\n",
    "    best_ada = ada_grid.best_estimator_\n",
    "    \n",
    "    EPE_ada = ada_grid.best_score_ #this value is given as the mean\n",
    "    print('Done with Ada Boost')\n",
    "    \n",
    "    # Random Forest\n",
    "    model_rf = RandomForestRegressor()\n",
    "    \n",
    "    params_rf = {'bootstrap': [True, False],\n",
    "                 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "                 'max_features': ['auto', 'sqrt'],\n",
    "                 'min_samples_leaf': [1, 2, 4],\n",
    "                 'min_samples_split': [2, 5, 10],\n",
    "                 'n_estimators': [50, 100, 200, 400, 600, 800, 1000]}\n",
    "    rf_grid = GridSearchCV(model_rf, params_rf, cv=InnerCV, scoring='neg_root_mean_squared_error').fit(X,y)\n",
    "    best_rf = rf_grid.best_estimator_\n",
    "    \n",
    "    EPE_rf = rf_grid.best_score_\n",
    "    print('Done with Random Forest')\n",
    "    \n",
    "    # Support Vector Regression\n",
    "    model_svr= svr_rbf = SVR()\n",
    "    \n",
    "    params_svr = parameters = {'kernel': ('linear', 'rbf','poly'), \n",
    "                              'C':[1.5, 10],\n",
    "                              'gamma': [1e-7, 1e-4],\n",
    "                              'epsilon':[0.1,0.2,0.5,0.3]}\n",
    "    svr_grid = GridSearchCV(model_svr, params_svr, cv=InnerCV, scoring='neg_root_mean_squared_error').fit(X,y)\n",
    "    best_svr = svr_grid.best_estimator_\n",
    "    \n",
    "    EPE_svr = svr_grid.best_score_\n",
    "    print('Done with Support Vector Regression Forest')\n",
    "\n",
    "    list_err = [EPE_bag, EPE_ada, EPE_rf, EPE_svr]\n",
    "    best_models = [best_bag, best_ada, best_rf,best_svr]\n",
    "    min_err = list_err.index(min(list_err))\n",
    "    \n",
    "    best = best_models[min_err]\n",
    "    \n",
    "    model = best.fit(X,y)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer fold is defined below - also where the model assesment is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPE_test = []\n",
    "EPE_train = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(CV.split(X,y)):\n",
    "    \n",
    "    print(f'Fold: {i}')\n",
    "    \n",
    "    X_train = X.iloc[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X.iloc[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    #############################\n",
    "    ### Best model - returned ###\n",
    "    #############################\n",
    "    \n",
    "    model = models(X_train, y_train, K) #K is the number of folds\n",
    "    yhat_test = model.predict(X_test)\n",
    "    yhat_train = model.predict(X_train)\n",
    "    \n",
    "    EPE_test.append(mean_squared_error(y_test, yhat_test, squared = False))\n",
    "    EPE_train.append(mean_squared_error(y_train, yhat_train, squared = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_eval = pd.DataFrame([EPE_test, EPE_train], columns=['RMSE test','RMSE train'])\n",
    "performance_eval.head(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPE = np.mean(EPE_test)\n",
    "model = models(X, y, K) #final model\n",
    "y_hat_new = model.predict(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save in .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('predictions_s184296s163724.txt', np.array(y_hat_new), fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('estimatedRMSE_s184296s163724.txt', np.array(EPE_test), fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
